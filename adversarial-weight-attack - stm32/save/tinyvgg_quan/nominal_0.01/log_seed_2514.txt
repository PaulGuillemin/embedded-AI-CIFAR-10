save path : ./save/tinyvgg_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 0, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 2514, 'save_path': './save/tinyvgg_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': False}
Random Seed: 2514
python version : 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]
torch  version : 2.8.0+cpu
cudnn  version : None
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU(inplace=True)
    (6): Dropout2d(p=0.09, inplace=False)
    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): ReLU(inplace=True)
    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (gap): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Sequential(
    (0): quan_Linear(in_features=32, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 06:37:30] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 19.561 (19.561)   Data 19.427 (19.427)   Loss 2.3258 (2.3258)   Prec@1 9.000 (9.000)   Prec@5 51.000 (51.000)   [2025-10-22 06:37:49]
  Epoch: [000][100/500]   Time 0.042 (0.236)   Data 0.001 (0.193)   Loss 1.9472 (1.9736)   Prec@1 27.000 (25.743)   Prec@5 81.000 (79.426)   [2025-10-22 06:37:53]
  Epoch: [000][200/500]   Time 0.042 (0.140)   Data 0.001 (0.097)   Loss 1.7457 (1.8418)   Prec@1 37.000 (30.761)   Prec@5 88.000 (83.493)   [2025-10-22 06:37:58]
  Epoch: [000][300/500]   Time 0.042 (0.107)   Data 0.001 (0.065)   Loss 1.6725 (1.7601)   Prec@1 41.000 (33.957)   Prec@5 90.000 (85.754)   [2025-10-22 06:38:02]
  Epoch: [000][400/500]   Time 0.041 (0.091)   Data 0.001 (0.049)   Loss 1.4281 (1.6995)   Prec@1 49.000 (36.367)   Prec@5 95.000 (87.160)   [2025-10-22 06:38:06]
  **Train** Prec@1 38.512 Prec@5 88.238 Error@1 61.488
  **Test** Prec@1 51.000 Prec@5 93.880 Error@1 49.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:38:31] [Epoch=001/040] [Need: 00:39:37] [LR=0.0100] [Best : Accuracy=51.00, Error=49.00]
  Epoch: [001][000/500]   Time 18.975 (18.975)   Data 18.923 (18.923)   Loss 1.3340 (1.3340)   Prec@1 50.000 (50.000)   Prec@5 95.000 (95.000)   [2025-10-22 06:38:50]
  Epoch: [001][100/500]   Time 0.044 (0.231)   Data 0.001 (0.188)   Loss 1.4184 (1.3838)   Prec@1 46.000 (49.921)   Prec@5 95.000 (93.109)   [2025-10-22 06:38:54]
  Epoch: [001][200/500]   Time 0.043 (0.140)   Data 0.001 (0.095)   Loss 1.2113 (1.3638)   Prec@1 61.000 (50.612)   Prec@5 92.000 (93.343)   [2025-10-22 06:38:59]
  Epoch: [001][300/500]   Time 0.047 (0.108)   Data 0.001 (0.064)   Loss 1.3830 (1.3437)   Prec@1 40.000 (51.422)   Prec@5 95.000 (93.591)   [2025-10-22 06:39:03]
  Epoch: [001][400/500]   Time 0.039 (0.092)   Data 0.001 (0.048)   Loss 1.1883 (1.3318)   Prec@1 55.000 (51.776)   Prec@5 96.000 (93.736)   [2025-10-22 06:39:07]
  **Train** Prec@1 52.202 Prec@5 93.856 Error@1 47.798
  **Test** Prec@1 56.310 Prec@5 95.370 Error@1 43.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:39:33] [Epoch=002/040] [Need: 00:39:09] [LR=0.0100] [Best : Accuracy=56.31, Error=43.69]
  Epoch: [002][000/500]   Time 19.373 (19.373)   Data 19.323 (19.323)   Loss 1.2564 (1.2564)   Prec@1 57.000 (57.000)   Prec@5 95.000 (95.000)   [2025-10-22 06:39:53]
  Epoch: [002][100/500]   Time 0.043 (0.234)   Data 0.001 (0.192)   Loss 1.1605 (1.2231)   Prec@1 60.000 (56.366)   Prec@5 96.000 (94.624)   [2025-10-22 06:39:57]
  Epoch: [002][200/500]   Time 0.043 (0.140)   Data 0.001 (0.097)   Loss 1.0708 (1.2199)   Prec@1 60.000 (56.219)   Prec@5 97.000 (94.701)   [2025-10-22 06:40:02]
  Epoch: [002][300/500]   Time 0.047 (0.109)   Data 0.001 (0.065)   Loss 1.1535 (1.2090)   Prec@1 56.000 (56.585)   Prec@5 97.000 (94.963)   [2025-10-22 06:40:06]
  Epoch: [002][400/500]   Time 0.041 (0.093)   Data 0.001 (0.049)   Loss 1.1542 (1.1974)   Prec@1 58.000 (57.212)   Prec@5 94.000 (95.055)   [2025-10-22 06:40:11]
  **Train** Prec@1 57.304 Prec@5 95.136 Error@1 42.696
  **Test** Prec@1 60.420 Prec@5 96.380 Error@1 39.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:40:35] [Epoch=003/040] [Need: 00:38:07] [LR=0.0100] [Best : Accuracy=60.42, Error=39.58]
  Epoch: [003][000/500]   Time 19.055 (19.055)   Data 18.991 (18.991)   Loss 1.2338 (1.2338)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-22 06:40:54]
  Epoch: [003][100/500]   Time 0.042 (0.232)   Data 0.001 (0.189)   Loss 1.0093 (1.1462)   Prec@1 61.000 (58.881)   Prec@5 96.000 (95.168)   [2025-10-22 06:40:59]
  Epoch: [003][200/500]   Time 0.041 (0.139)   Data 0.001 (0.095)   Loss 0.9877 (1.1415)   Prec@1 60.000 (58.746)   Prec@5 97.000 (95.463)   [2025-10-22 06:41:03]
  Epoch: [003][300/500]   Time 0.048 (0.107)   Data 0.001 (0.064)   Loss 1.0695 (1.1330)   Prec@1 58.000 (59.033)   Prec@5 96.000 (95.651)   [2025-10-22 06:41:07]
  Epoch: [003][400/500]   Time 0.042 (0.092)   Data 0.001 (0.048)   Loss 0.9649 (1.1308)   Prec@1 69.000 (59.304)   Prec@5 98.000 (95.671)   [2025-10-22 06:41:12]
  **Train** Prec@1 59.582 Prec@5 95.710 Error@1 40.418
  **Test** Prec@1 64.010 Prec@5 96.960 Error@1 35.990
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:41:37] [Epoch=004/040] [Need: 00:37:04] [LR=0.0100] [Best : Accuracy=64.01, Error=35.99]
  Epoch: [004][000/500]   Time 33.217 (33.217)   Data 33.156 (33.156)   Loss 1.0764 (1.0764)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-22 06:42:10]
  Epoch: [004][100/500]   Time 0.048 (0.369)   Data 0.001 (0.329)   Loss 0.9963 (1.1037)   Prec@1 65.000 (60.337)   Prec@5 98.000 (95.812)   [2025-10-22 06:42:14]
  Epoch: [004][200/500]   Time 0.107 (0.219)   Data 0.001 (0.166)   Loss 1.1179 (1.0840)   Prec@1 59.000 (61.070)   Prec@5 96.000 (96.204)   [2025-10-22 06:42:21]
  Epoch: [004][300/500]   Time 0.053 (0.167)   Data 0.001 (0.112)   Loss 1.2301 (1.0793)   Prec@1 56.000 (61.239)   Prec@5 94.000 (96.193)   [2025-10-22 06:42:27]
  Epoch: [004][400/500]   Time 0.080 (0.141)   Data 0.001 (0.084)   Loss 0.9444 (1.0746)   Prec@1 71.000 (61.466)   Prec@5 94.000 (96.155)   [2025-10-22 06:42:33]
  **Train** Prec@1 61.536 Prec@5 96.138 Error@1 38.464
  **Test** Prec@1 63.100 Prec@5 96.720 Error@1 36.900

==>>[2025-10-22 06:43:17] [Epoch=005/040] [Need: 00:40:32] [LR=0.0100] [Best : Accuracy=64.01, Error=35.99]
  Epoch: [005][000/500]   Time 38.498 (38.498)   Data 38.445 (38.445)   Loss 1.0137 (1.0137)   Prec@1 65.000 (65.000)   Prec@5 97.000 (97.000)   [2025-10-22 06:43:56]
  Epoch: [005][100/500]   Time 0.047 (0.423)   Data 0.001 (0.382)   Loss 1.0899 (1.0543)   Prec@1 61.000 (63.099)   Prec@5 95.000 (96.505)   [2025-10-22 06:44:00]
  Epoch: [005][200/500]   Time 0.039 (0.233)   Data 0.001 (0.192)   Loss 0.8823 (1.0399)   Prec@1 71.000 (62.955)   Prec@5 100.000 (96.423)   [2025-10-22 06:44:04]
  Epoch: [005][300/500]   Time 0.080 (0.172)   Data 0.005 (0.129)   Loss 1.0675 (1.0325)   Prec@1 63.000 (63.169)   Prec@5 96.000 (96.512)   [2025-10-22 06:44:09]
  Epoch: [005][400/500]   Time 0.057 (0.145)   Data 0.002 (0.097)   Loss 0.9967 (1.0284)   Prec@1 62.000 (63.302)   Prec@5 98.000 (96.544)   [2025-10-22 06:44:15]
  **Train** Prec@1 63.238 Prec@5 96.518 Error@1 36.762
  **Test** Prec@1 66.110 Prec@5 97.320 Error@1 33.890
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:45:00] [Epoch=006/040] [Need: 00:42:29] [LR=0.0100] [Best : Accuracy=66.11, Error=33.89]
  Epoch: [006][000/500]   Time 38.087 (38.087)   Data 38.037 (38.037)   Loss 0.8248 (0.8248)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-22 06:45:38]
  Epoch: [006][100/500]   Time 0.042 (0.418)   Data 0.001 (0.378)   Loss 1.1006 (0.9971)   Prec@1 63.000 (64.614)   Prec@5 94.000 (96.535)   [2025-10-22 06:45:42]
  Epoch: [006][200/500]   Time 0.038 (0.231)   Data 0.001 (0.190)   Loss 0.7605 (0.9886)   Prec@1 75.000 (65.159)   Prec@5 97.000 (96.692)   [2025-10-22 06:45:46]
  Epoch: [006][300/500]   Time 0.038 (0.168)   Data 0.001 (0.127)   Loss 0.8675 (0.9941)   Prec@1 71.000 (64.701)   Prec@5 96.000 (96.741)   [2025-10-22 06:45:50]
  Epoch: [006][400/500]   Time 0.050 (0.140)   Data 0.001 (0.096)   Loss 0.8540 (0.9961)   Prec@1 73.000 (64.594)   Prec@5 97.000 (96.681)   [2025-10-22 06:45:56]
  **Train** Prec@1 64.752 Prec@5 96.694 Error@1 35.248
  **Test** Prec@1 68.020 Prec@5 97.300 Error@1 31.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:46:33] [Epoch=007/040] [Need: 00:42:41] [LR=0.0100] [Best : Accuracy=68.02, Error=31.98]
  Epoch: [007][000/500]   Time 37.849 (37.849)   Data 37.795 (37.795)   Loss 1.0490 (1.0490)   Prec@1 60.000 (60.000)   Prec@5 100.000 (100.000)   [2025-10-22 06:47:11]
  Epoch: [007][100/500]   Time 0.038 (0.416)   Data 0.001 (0.375)   Loss 0.9062 (0.9729)   Prec@1 69.000 (65.465)   Prec@5 99.000 (97.040)   [2025-10-22 06:47:15]
  Epoch: [007][200/500]   Time 0.036 (0.229)   Data 0.001 (0.189)   Loss 0.9058 (0.9739)   Prec@1 67.000 (65.502)   Prec@5 96.000 (96.930)   [2025-10-22 06:47:19]
  Epoch: [007][300/500]   Time 0.059 (0.169)   Data 0.001 (0.127)   Loss 1.0034 (0.9760)   Prec@1 63.000 (65.385)   Prec@5 97.000 (96.927)   [2025-10-22 06:47:24]
  Epoch: [007][400/500]   Time 0.056 (0.143)   Data 0.001 (0.095)   Loss 0.9909 (0.9750)   Prec@1 64.000 (65.377)   Prec@5 98.000 (96.970)   [2025-10-22 06:47:30]
  **Train** Prec@1 65.380 Prec@5 96.950 Error@1 34.620
  **Test** Prec@1 67.470 Prec@5 97.340 Error@1 32.530

==>>[2025-10-22 06:48:16] [Epoch=008/040] [Need: 00:43:03] [LR=0.0100] [Best : Accuracy=68.02, Error=31.98]
  Epoch: [008][000/500]   Time 37.924 (37.924)   Data 37.879 (37.879)   Loss 0.9533 (0.9533)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-22 06:48:54]
  Epoch: [008][100/500]   Time 0.044 (0.417)   Data 0.001 (0.376)   Loss 0.7887 (0.9649)   Prec@1 74.000 (66.059)   Prec@5 95.000 (96.980)   [2025-10-22 06:48:58]
  Epoch: [008][200/500]   Time 0.045 (0.230)   Data 0.001 (0.189)   Loss 1.0766 (0.9511)   Prec@1 66.000 (66.527)   Prec@5 96.000 (96.995)   [2025-10-22 06:49:02]
  Epoch: [008][300/500]   Time 0.038 (0.167)   Data 0.001 (0.127)   Loss 0.8541 (0.9467)   Prec@1 71.000 (66.681)   Prec@5 98.000 (97.076)   [2025-10-22 06:49:06]
  Epoch: [008][400/500]   Time 0.077 (0.139)   Data 0.002 (0.095)   Loss 0.8128 (0.9489)   Prec@1 71.000 (66.693)   Prec@5 95.000 (97.045)   [2025-10-22 06:49:12]
  **Train** Prec@1 66.716 Prec@5 97.086 Error@1 33.284
  **Test** Prec@1 68.950 Prec@5 97.690 Error@1 31.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:49:56] [Epoch=009/040] [Need: 00:42:50] [LR=0.0100] [Best : Accuracy=68.95, Error=31.05]
  Epoch: [009][000/500]   Time 38.637 (38.637)   Data 38.587 (38.587)   Loss 1.0437 (1.0437)   Prec@1 55.000 (55.000)   Prec@5 95.000 (95.000)   [2025-10-22 06:50:35]
  Epoch: [009][100/500]   Time 0.040 (0.423)   Data 0.001 (0.383)   Loss 1.0380 (0.9316)   Prec@1 62.000 (67.545)   Prec@5 96.000 (97.178)   [2025-10-22 06:50:39]
  Epoch: [009][200/500]   Time 0.041 (0.233)   Data 0.001 (0.193)   Loss 0.7709 (0.9243)   Prec@1 70.000 (67.642)   Prec@5 99.000 (97.323)   [2025-10-22 06:50:43]
  Epoch: [009][300/500]   Time 0.041 (0.169)   Data 0.001 (0.129)   Loss 1.1264 (0.9251)   Prec@1 64.000 (67.608)   Prec@5 94.000 (97.239)   [2025-10-22 06:50:47]
  Epoch: [009][400/500]   Time 0.052 (0.143)   Data 0.001 (0.097)   Loss 1.0630 (0.9224)   Prec@1 61.000 (67.526)   Prec@5 98.000 (97.254)   [2025-10-22 06:50:53]
  **Train** Prec@1 67.514 Prec@5 97.190 Error@1 32.486
  **Test** Prec@1 69.710 Prec@5 97.860 Error@1 30.290
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:51:38] [Epoch=010/040] [Need: 00:42:23] [LR=0.0100] [Best : Accuracy=69.71, Error=30.29]
  Epoch: [010][000/500]   Time 40.737 (40.737)   Data 40.681 (40.681)   Loss 0.8860 (0.8860)   Prec@1 72.000 (72.000)   Prec@5 99.000 (99.000)   [2025-10-22 06:52:18]
  Epoch: [010][100/500]   Time 0.040 (0.446)   Data 0.001 (0.404)   Loss 0.8045 (0.8941)   Prec@1 70.000 (68.545)   Prec@5 98.000 (97.386)   [2025-10-22 06:52:23]
  Epoch: [010][200/500]   Time 0.060 (0.253)   Data 0.001 (0.204)   Loss 0.9405 (0.9048)   Prec@1 70.000 (68.279)   Prec@5 97.000 (97.358)   [2025-10-22 06:52:29]
  Epoch: [010][300/500]   Time 0.048 (0.191)   Data 0.001 (0.137)   Loss 0.6483 (0.9059)   Prec@1 81.000 (68.199)   Prec@5 100.000 (97.329)   [2025-10-22 06:52:35]
  Epoch: [010][400/500]   Time 0.058 (0.161)   Data 0.002 (0.103)   Loss 0.8231 (0.9091)   Prec@1 70.000 (68.107)   Prec@5 98.000 (97.272)   [2025-10-22 06:52:42]
  **Train** Prec@1 68.152 Prec@5 97.272 Error@1 31.848
  **Test** Prec@1 69.960 Prec@5 97.760 Error@1 30.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:53:27] [Epoch=011/040] [Need: 00:42:04] [LR=0.0100] [Best : Accuracy=69.96, Error=30.04]
  Epoch: [011][000/500]   Time 37.674 (37.674)   Data 37.620 (37.620)   Loss 1.1545 (1.1545)   Prec@1 56.000 (56.000)   Prec@5 97.000 (97.000)   [2025-10-22 06:54:05]
  Epoch: [011][100/500]   Time 0.042 (0.413)   Data 0.001 (0.373)   Loss 0.7866 (0.8959)   Prec@1 72.000 (68.129)   Prec@5 98.000 (97.653)   [2025-10-22 06:54:09]
  Epoch: [011][200/500]   Time 0.045 (0.228)   Data 0.001 (0.188)   Loss 0.9054 (0.8935)   Prec@1 71.000 (68.507)   Prec@5 96.000 (97.512)   [2025-10-22 06:54:13]
  Epoch: [011][300/500]   Time 0.056 (0.167)   Data 0.001 (0.126)   Loss 0.8339 (0.8969)   Prec@1 73.000 (68.349)   Prec@5 98.000 (97.462)   [2025-10-22 06:54:18]
  Epoch: [011][400/500]   Time 0.057 (0.142)   Data 0.001 (0.095)   Loss 0.8381 (0.8910)   Prec@1 63.000 (68.426)   Prec@5 99.000 (97.526)   [2025-10-22 06:54:24]
  **Train** Prec@1 68.500 Prec@5 97.522 Error@1 31.500
  **Test** Prec@1 70.330 Prec@5 97.610 Error@1 29.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:55:09] [Epoch=012/040] [Need: 00:41:10] [LR=0.0100] [Best : Accuracy=70.33, Error=29.67]
  Epoch: [012][000/500]   Time 34.951 (34.951)   Data 34.901 (34.901)   Loss 0.9985 (0.9985)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-22 06:55:44]
  Epoch: [012][100/500]   Time 0.038 (0.387)   Data 0.001 (0.346)   Loss 0.8548 (0.8806)   Prec@1 70.000 (69.733)   Prec@5 95.000 (97.356)   [2025-10-22 06:55:48]
  Epoch: [012][200/500]   Time 0.040 (0.215)   Data 0.001 (0.175)   Loss 0.8821 (0.8817)   Prec@1 69.000 (69.577)   Prec@5 98.000 (97.318)   [2025-10-22 06:55:52]
  Epoch: [012][300/500]   Time 0.054 (0.160)   Data 0.001 (0.117)   Loss 0.8607 (0.8754)   Prec@1 65.000 (69.525)   Prec@5 99.000 (97.425)   [2025-10-22 06:55:57]
  Epoch: [012][400/500]   Time 0.051 (0.135)   Data 0.001 (0.088)   Loss 0.9239 (0.8794)   Prec@1 69.000 (69.454)   Prec@5 100.000 (97.449)   [2025-10-22 06:56:03]
  **Train** Prec@1 69.540 Prec@5 97.466 Error@1 30.460
  **Test** Prec@1 70.790 Prec@5 97.780 Error@1 29.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:56:38] [Epoch=013/040] [Need: 00:39:45] [LR=0.0100] [Best : Accuracy=70.79, Error=29.21]
  Epoch: [013][000/500]   Time 38.250 (38.250)   Data 38.200 (38.200)   Loss 1.0677 (1.0677)   Prec@1 63.000 (63.000)   Prec@5 95.000 (95.000)   [2025-10-22 06:57:17]
  Epoch: [013][100/500]   Time 0.043 (0.420)   Data 0.001 (0.379)   Loss 0.7504 (0.8684)   Prec@1 70.000 (69.861)   Prec@5 98.000 (97.535)   [2025-10-22 06:57:21]
  Epoch: [013][200/500]   Time 0.039 (0.232)   Data 0.001 (0.191)   Loss 0.6357 (0.8723)   Prec@1 78.000 (69.582)   Prec@5 99.000 (97.572)   [2025-10-22 06:57:25]
  Epoch: [013][300/500]   Time 0.059 (0.169)   Data 0.002 (0.128)   Loss 0.8869 (0.8714)   Prec@1 68.000 (69.405)   Prec@5 96.000 (97.588)   [2025-10-22 06:57:29]
  Epoch: [013][400/500]   Time 0.060 (0.142)   Data 0.002 (0.096)   Loss 0.8515 (0.8686)   Prec@1 74.000 (69.586)   Prec@5 96.000 (97.646)   [2025-10-22 06:57:35]
  **Train** Prec@1 69.560 Prec@5 97.626 Error@1 30.440
  **Test** Prec@1 71.500 Prec@5 98.240 Error@1 28.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 06:58:20] [Epoch=014/040] [Need: 00:38:41] [LR=0.0100] [Best : Accuracy=71.50, Error=28.50]
  Epoch: [014][000/500]   Time 38.378 (38.378)   Data 38.323 (38.323)   Loss 0.9258 (0.9258)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-22 06:58:58]
  Epoch: [014][100/500]   Time 0.044 (0.421)   Data 0.001 (0.380)   Loss 0.9320 (0.8577)   Prec@1 67.000 (70.089)   Prec@5 98.000 (97.683)   [2025-10-22 06:59:02]
  Epoch: [014][200/500]   Time 0.039 (0.233)   Data 0.001 (0.192)   Loss 0.9279 (0.8613)   Prec@1 74.000 (69.950)   Prec@5 94.000 (97.627)   [2025-10-22 06:59:07]
  Epoch: [014][300/500]   Time 0.047 (0.169)   Data 0.001 (0.128)   Loss 0.8448 (0.8609)   Prec@1 72.000 (70.090)   Prec@5 96.000 (97.605)   [2025-10-22 06:59:11]
  Epoch: [014][400/500]   Time 0.061 (0.140)   Data 0.001 (0.097)   Loss 0.8068 (0.8593)   Prec@1 72.000 (70.175)   Prec@5 99.000 (97.606)   [2025-10-22 06:59:16]
  **Train** Prec@1 70.240 Prec@5 97.606 Error@1 29.760
  **Test** Prec@1 72.500 Prec@5 98.140 Error@1 27.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:00:00] [Epoch=015/040] [Need: 00:37:29] [LR=0.0100] [Best : Accuracy=72.50, Error=27.50]
  Epoch: [015][000/500]   Time 33.366 (33.366)   Data 33.317 (33.317)   Loss 0.8605 (0.8605)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:00:33]
  Epoch: [015][100/500]   Time 0.037 (0.367)   Data 0.001 (0.330)   Loss 0.7316 (0.8497)   Prec@1 71.000 (70.455)   Prec@5 98.000 (97.861)   [2025-10-22 07:00:37]
  Epoch: [015][200/500]   Time 0.059 (0.211)   Data 0.001 (0.167)   Loss 0.7831 (0.8508)   Prec@1 73.000 (70.254)   Prec@5 98.000 (97.866)   [2025-10-22 07:00:42]
  Epoch: [015][300/500]   Time 0.067 (0.161)   Data 0.003 (0.112)   Loss 1.1335 (0.8455)   Prec@1 56.000 (70.575)   Prec@5 95.000 (97.794)   [2025-10-22 07:00:48]
  Epoch: [015][400/500]   Time 0.060 (0.137)   Data 0.001 (0.084)   Loss 0.8133 (0.8487)   Prec@1 69.000 (70.401)   Prec@5 97.000 (97.751)   [2025-10-22 07:00:55]
  **Train** Prec@1 70.446 Prec@5 97.734 Error@1 29.554
  **Test** Prec@1 72.240 Prec@5 98.190 Error@1 27.760

==>>[2025-10-22 07:01:39] [Epoch=016/040] [Need: 00:36:13] [LR=0.0100] [Best : Accuracy=72.50, Error=27.50]
  Epoch: [016][000/500]   Time 37.972 (37.972)   Data 37.910 (37.910)   Loss 0.8649 (0.8649)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:02:17]
  Epoch: [016][100/500]   Time 0.038 (0.417)   Data 0.001 (0.376)   Loss 0.7572 (0.8396)   Prec@1 72.000 (70.812)   Prec@5 100.000 (98.010)   [2025-10-22 07:02:21]
  Epoch: [016][200/500]   Time 0.041 (0.230)   Data 0.001 (0.190)   Loss 0.7418 (0.8483)   Prec@1 72.000 (70.463)   Prec@5 99.000 (97.751)   [2025-10-22 07:02:25]
  Epoch: [016][300/500]   Time 0.040 (0.167)   Data 0.001 (0.127)   Loss 0.7765 (0.8430)   Prec@1 75.000 (70.581)   Prec@5 97.000 (97.860)   [2025-10-22 07:02:29]
  Epoch: [016][400/500]   Time 0.096 (0.138)   Data 0.001 (0.096)   Loss 0.8135 (0.8392)   Prec@1 72.000 (70.621)   Prec@5 98.000 (97.835)   [2025-10-22 07:02:34]
  **Train** Prec@1 70.590 Prec@5 97.800 Error@1 29.410
  **Test** Prec@1 73.050 Prec@5 98.250 Error@1 26.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:03:18] [Epoch=017/040] [Need: 00:34:54] [LR=0.0100] [Best : Accuracy=73.05, Error=26.95]
  Epoch: [017][000/500]   Time 37.794 (37.794)   Data 37.744 (37.744)   Loss 0.8322 (0.8322)   Prec@1 76.000 (76.000)   Prec@5 96.000 (96.000)   [2025-10-22 07:03:56]
  Epoch: [017][100/500]   Time 0.038 (0.415)   Data 0.001 (0.375)   Loss 0.7248 (0.8273)   Prec@1 72.000 (70.653)   Prec@5 98.000 (97.851)   [2025-10-22 07:04:00]
  Epoch: [017][200/500]   Time 0.040 (0.229)   Data 0.001 (0.189)   Loss 0.6294 (0.8242)   Prec@1 80.000 (71.184)   Prec@5 98.000 (97.692)   [2025-10-22 07:04:04]
  Epoch: [017][300/500]   Time 0.040 (0.166)   Data 0.001 (0.126)   Loss 0.8750 (0.8270)   Prec@1 71.000 (71.106)   Prec@5 98.000 (97.708)   [2025-10-22 07:04:08]
  Epoch: [017][400/500]   Time 0.050 (0.136)   Data 0.002 (0.095)   Loss 0.7616 (0.8265)   Prec@1 70.000 (71.180)   Prec@5 97.000 (97.723)   [2025-10-22 07:04:13]
  **Train** Prec@1 71.100 Prec@5 97.752 Error@1 28.900
  **Test** Prec@1 73.500 Prec@5 98.190 Error@1 26.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:04:57] [Epoch=018/040] [Need: 00:33:32] [LR=0.0100] [Best : Accuracy=73.50, Error=26.50]
  Epoch: [018][000/500]   Time 37.974 (37.974)   Data 37.924 (37.924)   Loss 1.0306 (1.0306)   Prec@1 60.000 (60.000)   Prec@5 98.000 (98.000)   [2025-10-22 07:05:35]
  Epoch: [018][100/500]   Time 0.043 (0.418)   Data 0.001 (0.376)   Loss 0.7252 (0.8345)   Prec@1 74.000 (70.653)   Prec@5 99.000 (97.624)   [2025-10-22 07:05:39]
  Epoch: [018][200/500]   Time 0.038 (0.230)   Data 0.001 (0.190)   Loss 0.9695 (0.8188)   Prec@1 64.000 (71.527)   Prec@5 99.000 (97.776)   [2025-10-22 07:05:43]
  Epoch: [018][300/500]   Time 0.038 (0.167)   Data 0.001 (0.127)   Loss 0.8576 (0.8253)   Prec@1 74.000 (71.332)   Prec@5 99.000 (97.734)   [2025-10-22 07:05:47]
  Epoch: [018][400/500]   Time 0.047 (0.138)   Data 0.002 (0.096)   Loss 0.8044 (0.8242)   Prec@1 72.000 (71.264)   Prec@5 96.000 (97.753)   [2025-10-22 07:05:52]
  **Train** Prec@1 71.236 Prec@5 97.792 Error@1 28.764
  **Test** Prec@1 73.380 Prec@5 98.210 Error@1 26.620

==>>[2025-10-22 07:06:37] [Epoch=019/040] [Need: 00:32:11] [LR=0.0100] [Best : Accuracy=73.50, Error=26.50]
  Epoch: [019][000/500]   Time 38.335 (38.335)   Data 38.280 (38.280)   Loss 0.7500 (0.7500)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:07:16]
  Epoch: [019][100/500]   Time 0.039 (0.420)   Data 0.001 (0.380)   Loss 1.0441 (0.8162)   Prec@1 68.000 (72.030)   Prec@5 93.000 (97.743)   [2025-10-22 07:07:20]
  Epoch: [019][200/500]   Time 0.039 (0.231)   Data 0.001 (0.191)   Loss 0.8190 (0.8189)   Prec@1 70.000 (71.537)   Prec@5 98.000 (97.826)   [2025-10-22 07:07:24]
  Epoch: [019][300/500]   Time 0.039 (0.168)   Data 0.001 (0.128)   Loss 1.0462 (0.8143)   Prec@1 63.000 (71.641)   Prec@5 98.000 (97.834)   [2025-10-22 07:07:28]
  Epoch: [019][400/500]   Time 0.057 (0.140)   Data 0.003 (0.097)   Loss 0.8161 (0.8136)   Prec@1 71.000 (71.556)   Prec@5 98.000 (97.885)   [2025-10-22 07:07:34]
  **Train** Prec@1 71.488 Prec@5 97.940 Error@1 28.512
  **Test** Prec@1 74.360 Prec@5 98.230 Error@1 25.640
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:08:17] [Epoch=020/040] [Need: 00:30:47] [LR=0.0100] [Best : Accuracy=74.36, Error=25.64]
  Epoch: [020][000/500]   Time 38.284 (38.284)   Data 38.227 (38.227)   Loss 0.6360 (0.6360)   Prec@1 80.000 (80.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:08:56]
  Epoch: [020][100/500]   Time 0.040 (0.421)   Data 0.001 (0.379)   Loss 1.0065 (0.8113)   Prec@1 64.000 (71.574)   Prec@5 96.000 (97.792)   [2025-10-22 07:09:00]
  Epoch: [020][200/500]   Time 0.047 (0.233)   Data 0.001 (0.191)   Loss 0.9189 (0.8122)   Prec@1 69.000 (71.488)   Prec@5 99.000 (97.841)   [2025-10-22 07:09:04]
  Epoch: [020][300/500]   Time 0.043 (0.169)   Data 0.001 (0.128)   Loss 0.7750 (0.8110)   Prec@1 69.000 (71.535)   Prec@5 100.000 (97.854)   [2025-10-22 07:09:08]
  Epoch: [020][400/500]   Time 0.057 (0.143)   Data 0.001 (0.096)   Loss 0.8993 (0.8054)   Prec@1 71.000 (71.761)   Prec@5 98.000 (97.918)   [2025-10-22 07:09:15]
  **Train** Prec@1 71.706 Prec@5 97.928 Error@1 28.294
  **Test** Prec@1 74.200 Prec@5 98.350 Error@1 25.800

==>>[2025-10-22 07:10:07] [Epoch=021/040] [Need: 00:29:30] [LR=0.0100] [Best : Accuracy=74.36, Error=25.64]
  Epoch: [021][000/500]   Time 76.911 (76.911)   Data 76.775 (76.775)   Loss 0.7629 (0.7629)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:11:24]
  Epoch: [021][100/500]   Time 0.115 (0.875)   Data 0.002 (0.762)   Loss 0.8511 (0.8042)   Prec@1 66.000 (72.059)   Prec@5 99.000 (97.861)   [2025-10-22 07:11:36]
  Epoch: [021][200/500]   Time 0.150 (0.501)   Data 0.002 (0.384)   Loss 0.8281 (0.8038)   Prec@1 73.000 (71.831)   Prec@5 100.000 (97.915)   [2025-10-22 07:11:48]
  Epoch: [021][300/500]   Time 0.106 (0.374)   Data 0.002 (0.257)   Loss 0.7130 (0.8057)   Prec@1 70.000 (71.767)   Prec@5 100.000 (97.963)   [2025-10-22 07:12:00]
  Epoch: [021][400/500]   Time 0.110 (0.310)   Data 0.002 (0.193)   Loss 0.9335 (0.8043)   Prec@1 70.000 (71.975)   Prec@5 98.000 (97.935)   [2025-10-22 07:12:12]
  **Train** Prec@1 71.908 Prec@5 97.918 Error@1 28.092
  **Test** Prec@1 73.800 Prec@5 98.300 Error@1 26.200

==>>[2025-10-22 07:13:41] [Epoch=022/040] [Need: 00:29:36] [LR=0.0100] [Best : Accuracy=74.36, Error=25.64]
  Epoch: [022][000/500]   Time 77.859 (77.859)   Data 77.729 (77.729)   Loss 0.8930 (0.8930)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-22 07:14:59]
  Epoch: [022][100/500]   Time 0.119 (0.880)   Data 0.002 (0.771)   Loss 0.7482 (0.8136)   Prec@1 71.000 (71.347)   Prec@5 99.000 (97.733)   [2025-10-22 07:15:10]
  Epoch: [022][200/500]   Time 0.115 (0.496)   Data 0.002 (0.388)   Loss 0.8158 (0.8025)   Prec@1 69.000 (72.005)   Prec@5 96.000 (97.900)   [2025-10-22 07:15:21]
  Epoch: [022][300/500]   Time 0.112 (0.370)   Data 0.002 (0.260)   Loss 0.6963 (0.7996)   Prec@1 76.000 (71.937)   Prec@5 100.000 (97.927)   [2025-10-22 07:15:33]
  Epoch: [022][400/500]   Time 0.116 (0.308)   Data 0.002 (0.196)   Loss 1.1039 (0.7981)   Prec@1 63.000 (72.050)   Prec@5 95.000 (97.928)   [2025-10-22 07:15:45]
  **Train** Prec@1 72.064 Prec@5 97.954 Error@1 27.936
  **Test** Prec@1 74.770 Prec@5 98.250 Error@1 25.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:17:15] [Epoch=023/040] [Need: 00:29:22] [LR=0.0100] [Best : Accuracy=74.77, Error=25.23]
  Epoch: [023][000/500]   Time 76.655 (76.655)   Data 76.531 (76.531)   Loss 0.7124 (0.7124)   Prec@1 77.000 (77.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:18:32]
  Epoch: [023][100/500]   Time 0.111 (0.866)   Data 0.002 (0.759)   Loss 0.9218 (0.8019)   Prec@1 69.000 (72.356)   Prec@5 95.000 (97.881)   [2025-10-22 07:18:43]
  Epoch: [023][200/500]   Time 0.109 (0.489)   Data 0.002 (0.383)   Loss 0.7458 (0.7917)   Prec@1 79.000 (72.816)   Prec@5 98.000 (97.935)   [2025-10-22 07:18:54]
  Epoch: [023][300/500]   Time 0.136 (0.363)   Data 0.002 (0.256)   Loss 0.7076 (0.7954)   Prec@1 70.000 (72.578)   Prec@5 99.000 (97.867)   [2025-10-22 07:19:04]
  Epoch: [023][400/500]   Time 0.137 (0.301)   Data 0.002 (0.193)   Loss 0.7173 (0.7958)   Prec@1 76.000 (72.466)   Prec@5 98.000 (97.833)   [2025-10-22 07:19:16]
  **Train** Prec@1 72.366 Prec@5 97.830 Error@1 27.634
  **Test** Prec@1 73.100 Prec@5 98.230 Error@1 26.900

==>>[2025-10-22 07:20:46] [Epoch=024/040] [Need: 00:28:50] [LR=0.0100] [Best : Accuracy=74.77, Error=25.23]
  Epoch: [024][000/500]   Time 77.741 (77.741)   Data 77.621 (77.621)   Loss 0.8462 (0.8462)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-22 07:22:04]
  Epoch: [024][100/500]   Time 0.104 (0.878)   Data 0.002 (0.770)   Loss 0.8645 (0.7886)   Prec@1 71.000 (72.871)   Prec@5 96.000 (98.010)   [2025-10-22 07:22:15]
  Epoch: [024][200/500]   Time 0.107 (0.497)   Data 0.002 (0.388)   Loss 0.7608 (0.7840)   Prec@1 71.000 (72.871)   Prec@5 98.000 (98.020)   [2025-10-22 07:22:26]
  Epoch: [024][300/500]   Time 0.106 (0.368)   Data 0.002 (0.260)   Loss 0.9048 (0.7896)   Prec@1 72.000 (72.718)   Prec@5 95.000 (97.983)   [2025-10-22 07:22:37]
  Epoch: [024][400/500]   Time 0.136 (0.304)   Data 0.002 (0.195)   Loss 0.8567 (0.7862)   Prec@1 69.000 (72.748)   Prec@5 99.000 (98.017)   [2025-10-22 07:22:48]
  **Train** Prec@1 72.638 Prec@5 98.008 Error@1 27.362
  **Test** Prec@1 74.580 Prec@5 98.400 Error@1 25.420

==>>[2025-10-22 07:24:17] [Epoch=025/040] [Need: 00:28:04] [LR=0.0010] [Best : Accuracy=74.77, Error=25.23]
  Epoch: [025][000/500]   Time 68.237 (68.237)   Data 68.093 (68.093)   Loss 0.9187 (0.9187)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-22 07:25:25]
  Epoch: [025][100/500]   Time 0.106 (0.785)   Data 0.002 (0.676)   Loss 0.7687 (0.7589)   Prec@1 71.000 (73.525)   Prec@5 96.000 (98.089)   [2025-10-22 07:25:36]
  Epoch: [025][200/500]   Time 0.112 (0.449)   Data 0.001 (0.341)   Loss 0.7081 (0.7519)   Prec@1 80.000 (73.761)   Prec@5 98.000 (98.095)   [2025-10-22 07:25:47]
  Epoch: [025][300/500]   Time 0.103 (0.336)   Data 0.002 (0.228)   Loss 0.5988 (0.7477)   Prec@1 77.000 (73.950)   Prec@5 100.000 (98.143)   [2025-10-22 07:25:58]
  Epoch: [025][400/500]   Time 0.132 (0.281)   Data 0.002 (0.172)   Loss 0.6801 (0.7459)   Prec@1 82.000 (73.950)   Prec@5 99.000 (98.177)   [2025-10-22 07:26:09]
  **Train** Prec@1 74.000 Prec@5 98.182 Error@1 26.000
  **Test** Prec@1 76.220 Prec@5 98.600 Error@1 23.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:27:38] [Epoch=026/040] [Need: 00:26:59] [LR=0.0010] [Best : Accuracy=76.22, Error=23.78]
  Epoch: [026][000/500]   Time 52.554 (52.554)   Data 52.493 (52.493)   Loss 0.7621 (0.7621)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-22 07:28:30]
  Epoch: [026][100/500]   Time 0.041 (0.564)   Data 0.001 (0.521)   Loss 0.7687 (0.7233)   Prec@1 75.000 (75.020)   Prec@5 97.000 (98.366)   [2025-10-22 07:28:35]
  Epoch: [026][200/500]   Time 0.038 (0.303)   Data 0.001 (0.262)   Loss 0.5907 (0.7262)   Prec@1 79.000 (74.697)   Prec@5 98.000 (98.323)   [2025-10-22 07:28:39]
  Epoch: [026][300/500]   Time 0.039 (0.216)   Data 0.001 (0.175)   Loss 0.7970 (0.7303)   Prec@1 70.000 (74.561)   Prec@5 99.000 (98.292)   [2025-10-22 07:28:43]
  Epoch: [026][400/500]   Time 0.039 (0.172)   Data 0.001 (0.132)   Loss 0.7015 (0.7289)   Prec@1 73.000 (74.561)   Prec@5 99.000 (98.344)   [2025-10-22 07:28:47]
  **Train** Prec@1 74.578 Prec@5 98.328 Error@1 25.422
  **Test** Prec@1 76.640 Prec@5 98.580 Error@1 23.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:30:05] [Epoch=027/040] [Need: 00:25:19] [LR=0.0010] [Best : Accuracy=76.64, Error=23.36]
  Epoch: [027][000/500]   Time 70.566 (70.566)   Data 70.510 (70.510)   Loss 0.7171 (0.7171)   Prec@1 80.000 (80.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:31:16]
  Epoch: [027][100/500]   Time 0.057 (0.748)   Data 0.001 (0.699)   Loss 0.9310 (0.7265)   Prec@1 65.000 (75.158)   Prec@5 98.000 (98.119)   [2025-10-22 07:31:21]
  Epoch: [027][200/500]   Time 0.051 (0.402)   Data 0.001 (0.352)   Loss 0.6006 (0.7196)   Prec@1 77.000 (75.035)   Prec@5 99.000 (98.269)   [2025-10-22 07:31:26]
  Epoch: [027][300/500]   Time 0.053 (0.287)   Data 0.001 (0.236)   Loss 0.7425 (0.7180)   Prec@1 80.000 (75.043)   Prec@5 99.000 (98.322)   [2025-10-22 07:31:32]
  Epoch: [027][400/500]   Time 0.040 (0.227)   Data 0.001 (0.177)   Loss 0.9674 (0.7207)   Prec@1 63.000 (74.873)   Prec@5 97.000 (98.339)   [2025-10-22 07:31:36]
  **Train** Prec@1 74.806 Prec@5 98.342 Error@1 25.194
  **Test** Prec@1 76.780 Prec@5 98.570 Error@1 23.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:32:02] [Epoch=028/040] [Need: 00:23:22] [LR=0.0010] [Best : Accuracy=76.78, Error=23.22]
  Epoch: [028][000/500]   Time 20.380 (20.380)   Data 20.323 (20.323)   Loss 0.7625 (0.7625)   Prec@1 72.000 (72.000)   Prec@5 100.000 (100.000)   [2025-10-22 07:32:23]
  Epoch: [028][100/500]   Time 0.056 (0.255)   Data 0.001 (0.202)   Loss 0.6600 (0.7224)   Prec@1 76.000 (74.703)   Prec@5 100.000 (98.149)   [2025-10-22 07:32:28]
  Epoch: [028][200/500]   Time 0.070 (0.167)   Data 0.001 (0.103)   Loss 0.5433 (0.7208)   Prec@1 85.000 (74.920)   Prec@5 100.000 (98.264)   [2025-10-22 07:32:36]
  Epoch: [028][300/500]   Time 0.061 (0.137)   Data 0.002 (0.069)   Loss 0.7305 (0.7199)   Prec@1 73.000 (75.003)   Prec@5 98.000 (98.306)   [2025-10-22 07:32:44]
  Epoch: [028][400/500]   Time 0.071 (0.122)   Data 0.003 (0.052)   Loss 0.7011 (0.7196)   Prec@1 75.000 (75.062)   Prec@5 99.000 (98.279)   [2025-10-22 07:32:51]
  **Train** Prec@1 74.968 Prec@5 98.330 Error@1 25.032
  **Test** Prec@1 76.580 Prec@5 98.610 Error@1 23.420

==>>[2025-10-22 07:34:03] [Epoch=029/040] [Need: 00:21:27] [LR=0.0010] [Best : Accuracy=76.78, Error=23.22]
  Epoch: [029][000/500]   Time 62.311 (62.311)   Data 62.249 (62.249)   Loss 0.8406 (0.8406)   Prec@1 69.000 (69.000)   Prec@5 100.000 (100.000)   [2025-10-22 07:35:05]
  Epoch: [029][100/500]   Time 0.035 (0.656)   Data 0.001 (0.617)   Loss 0.6062 (0.7395)   Prec@1 74.000 (74.416)   Prec@5 99.000 (98.168)   [2025-10-22 07:35:09]
  Epoch: [029][200/500]   Time 0.037 (0.349)   Data 0.001 (0.310)   Loss 0.7487 (0.7230)   Prec@1 76.000 (75.025)   Prec@5 99.000 (98.313)   [2025-10-22 07:35:13]
  Epoch: [029][300/500]   Time 0.038 (0.245)   Data 0.001 (0.207)   Loss 0.5597 (0.7231)   Prec@1 84.000 (74.834)   Prec@5 100.000 (98.395)   [2025-10-22 07:35:17]
  Epoch: [029][400/500]   Time 0.036 (0.193)   Data 0.001 (0.156)   Loss 0.6234 (0.7216)   Prec@1 79.000 (74.813)   Prec@5 96.000 (98.419)   [2025-10-22 07:35:20]
  **Train** Prec@1 74.976 Prec@5 98.464 Error@1 25.024
  **Test** Prec@1 76.680 Prec@5 98.530 Error@1 23.320

==>>[2025-10-22 07:35:49] [Epoch=030/040] [Need: 00:19:26] [LR=0.0010] [Best : Accuracy=76.78, Error=23.22]
  Epoch: [030][000/500]   Time 18.234 (18.234)   Data 18.184 (18.184)   Loss 0.6586 (0.6586)   Prec@1 82.000 (82.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:36:07]
  Epoch: [030][100/500]   Time 0.037 (0.218)   Data 0.001 (0.181)   Loss 0.6392 (0.7084)   Prec@1 81.000 (75.485)   Prec@5 98.000 (98.386)   [2025-10-22 07:36:11]
  Epoch: [030][200/500]   Time 0.045 (0.132)   Data 0.002 (0.091)   Loss 0.6541 (0.7043)   Prec@1 76.000 (75.274)   Prec@5 98.000 (98.428)   [2025-10-22 07:36:15]
  Epoch: [030][300/500]   Time 0.045 (0.104)   Data 0.001 (0.061)   Loss 0.6772 (0.7084)   Prec@1 76.000 (75.163)   Prec@5 98.000 (98.449)   [2025-10-22 07:36:20]
  Epoch: [030][400/500]   Time 0.045 (0.089)   Data 0.001 (0.046)   Loss 0.6415 (0.7105)   Prec@1 77.000 (75.070)   Prec@5 98.000 (98.421)   [2025-10-22 07:36:24]
  **Train** Prec@1 74.990 Prec@5 98.390 Error@1 25.010
  **Test** Prec@1 76.770 Prec@5 98.570 Error@1 23.230

==>>[2025-10-22 07:36:50] [Epoch=031/040] [Need: 00:17:13] [LR=0.0010] [Best : Accuracy=76.78, Error=23.22]
  Epoch: [031][000/500]   Time 19.385 (19.385)   Data 19.333 (19.333)   Loss 0.5818 (0.5818)   Prec@1 79.000 (79.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:37:09]
  Epoch: [031][100/500]   Time 0.050 (0.237)   Data 0.001 (0.192)   Loss 0.5882 (0.7098)   Prec@1 80.000 (75.168)   Prec@5 99.000 (98.446)   [2025-10-22 07:37:14]
  Epoch: [031][200/500]   Time 0.044 (0.142)   Data 0.001 (0.097)   Loss 0.6619 (0.7097)   Prec@1 82.000 (75.239)   Prec@5 98.000 (98.463)   [2025-10-22 07:37:18]
  Epoch: [031][300/500]   Time 0.043 (0.110)   Data 0.001 (0.065)   Loss 0.8597 (0.7181)   Prec@1 72.000 (74.890)   Prec@5 98.000 (98.382)   [2025-10-22 07:37:23]
  Epoch: [031][400/500]   Time 0.045 (0.094)   Data 0.001 (0.049)   Loss 0.8266 (0.7116)   Prec@1 74.000 (75.187)   Prec@5 99.000 (98.411)   [2025-10-22 07:37:27]
  **Train** Prec@1 75.214 Prec@5 98.400 Error@1 24.786
  **Test** Prec@1 77.070 Prec@5 98.590 Error@1 22.930
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:37:53] [Epoch=032/040] [Need: 00:15:05] [LR=0.0010] [Best : Accuracy=77.07, Error=22.93]
  Epoch: [032][000/500]   Time 19.585 (19.585)   Data 19.532 (19.532)   Loss 0.5681 (0.5681)   Prec@1 78.000 (78.000)   Prec@5 98.000 (98.000)   [2025-10-22 07:38:12]
  Epoch: [032][100/500]   Time 0.045 (0.238)   Data 0.001 (0.194)   Loss 0.6670 (0.7111)   Prec@1 79.000 (75.564)   Prec@5 98.000 (98.307)   [2025-10-22 07:38:17]
  Epoch: [032][200/500]   Time 0.045 (0.142)   Data 0.001 (0.098)   Loss 0.7362 (0.7024)   Prec@1 77.000 (75.552)   Prec@5 97.000 (98.368)   [2025-10-22 07:38:21]
  Epoch: [032][300/500]   Time 0.053 (0.109)   Data 0.001 (0.066)   Loss 0.8281 (0.7045)   Prec@1 74.000 (75.488)   Prec@5 98.000 (98.359)   [2025-10-22 07:38:26]
  Epoch: [032][400/500]   Time 0.041 (0.093)   Data 0.001 (0.050)   Loss 0.9437 (0.7081)   Prec@1 70.000 (75.334)   Prec@5 95.000 (98.354)   [2025-10-22 07:38:30]
  **Train** Prec@1 75.382 Prec@5 98.344 Error@1 24.618
  **Test** Prec@1 76.750 Prec@5 98.670 Error@1 23.250

==>>[2025-10-22 07:39:03] [Epoch=033/040] [Need: 00:13:03] [LR=0.0010] [Best : Accuracy=77.07, Error=22.93]
  Epoch: [033][000/500]   Time 19.105 (19.105)   Data 19.057 (19.057)   Loss 0.5369 (0.5369)   Prec@1 79.000 (79.000)   Prec@5 100.000 (100.000)   [2025-10-22 07:39:22]
  Epoch: [033][100/500]   Time 0.043 (0.234)   Data 0.001 (0.190)   Loss 0.4881 (0.7277)   Prec@1 82.000 (74.713)   Prec@5 99.000 (98.455)   [2025-10-22 07:39:27]
  Epoch: [033][200/500]   Time 0.042 (0.140)   Data 0.001 (0.096)   Loss 0.7327 (0.7228)   Prec@1 74.000 (74.891)   Prec@5 100.000 (98.363)   [2025-10-22 07:39:31]
  Epoch: [033][300/500]   Time 0.044 (0.108)   Data 0.001 (0.064)   Loss 0.8124 (0.7166)   Prec@1 70.000 (75.249)   Prec@5 100.000 (98.346)   [2025-10-22 07:39:36]
  Epoch: [033][400/500]   Time 0.052 (0.092)   Data 0.001 (0.048)   Loss 0.6446 (0.7154)   Prec@1 83.000 (75.307)   Prec@5 99.000 (98.392)   [2025-10-22 07:39:40]
  **Train** Prec@1 75.378 Prec@5 98.394 Error@1 24.622
  **Test** Prec@1 77.100 Prec@5 98.700 Error@1 22.900
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:40:06] [Epoch=034/040] [Need: 00:11:02] [LR=0.0010] [Best : Accuracy=77.10, Error=22.90]
  Epoch: [034][000/500]   Time 19.328 (19.328)   Data 19.279 (19.279)   Loss 0.7739 (0.7739)   Prec@1 73.000 (73.000)   Prec@5 98.000 (98.000)   [2025-10-22 07:40:25]
  Epoch: [034][100/500]   Time 0.043 (0.236)   Data 0.001 (0.192)   Loss 0.5542 (0.7007)   Prec@1 82.000 (75.871)   Prec@5 98.000 (98.267)   [2025-10-22 07:40:29]
  Epoch: [034][200/500]   Time 0.041 (0.141)   Data 0.001 (0.097)   Loss 0.8333 (0.7093)   Prec@1 66.000 (75.244)   Prec@5 100.000 (98.383)   [2025-10-22 07:40:34]
  Epoch: [034][300/500]   Time 0.043 (0.109)   Data 0.001 (0.065)   Loss 0.6466 (0.7072)   Prec@1 79.000 (75.236)   Prec@5 99.000 (98.382)   [2025-10-22 07:40:38]
  Epoch: [034][400/500]   Time 0.041 (0.093)   Data 0.001 (0.049)   Loss 0.8114 (0.7050)   Prec@1 76.000 (75.377)   Prec@5 99.000 (98.357)   [2025-10-22 07:40:43]
  **Train** Prec@1 75.284 Prec@5 98.390 Error@1 24.716
  **Test** Prec@1 77.070 Prec@5 98.640 Error@1 22.930

==>>[2025-10-22 07:41:08] [Epoch=035/040] [Need: 00:09:05] [LR=0.0010] [Best : Accuracy=77.10, Error=22.90]
  Epoch: [035][000/500]   Time 19.438 (19.438)   Data 19.386 (19.386)   Loss 0.5271 (0.5271)   Prec@1 81.000 (81.000)   Prec@5 100.000 (100.000)   [2025-10-22 07:41:28]
  Epoch: [035][100/500]   Time 0.042 (0.237)   Data 0.001 (0.193)   Loss 0.9061 (0.7159)   Prec@1 72.000 (74.693)   Prec@5 98.000 (98.386)   [2025-10-22 07:41:32]
  Epoch: [035][200/500]   Time 0.053 (0.141)   Data 0.001 (0.097)   Loss 0.6582 (0.7051)   Prec@1 78.000 (75.184)   Prec@5 99.000 (98.358)   [2025-10-22 07:41:37]
  Epoch: [035][300/500]   Time 0.056 (0.109)   Data 0.001 (0.065)   Loss 0.6671 (0.7067)   Prec@1 77.000 (75.282)   Prec@5 97.000 (98.385)   [2025-10-22 07:41:41]
  Epoch: [035][400/500]   Time 0.060 (0.095)   Data 0.001 (0.049)   Loss 0.7292 (0.7053)   Prec@1 75.000 (75.272)   Prec@5 98.000 (98.434)   [2025-10-22 07:41:46]
  **Train** Prec@1 75.272 Prec@5 98.418 Error@1 24.728
  **Test** Prec@1 77.150 Prec@5 98.660 Error@1 22.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:42:13] [Epoch=036/040] [Need: 00:07:11] [LR=0.0010] [Best : Accuracy=77.15, Error=22.85]
  Epoch: [036][000/500]   Time 28.749 (28.749)   Data 28.682 (28.682)   Loss 0.7224 (0.7224)   Prec@1 75.000 (75.000)   Prec@5 96.000 (96.000)   [2025-10-22 07:42:41]
  Epoch: [036][100/500]   Time 0.033 (0.321)   Data 0.001 (0.285)   Loss 0.6197 (0.6991)   Prec@1 77.000 (75.743)   Prec@5 99.000 (98.455)   [2025-10-22 07:42:45]
  Epoch: [036][200/500]   Time 0.043 (0.182)   Data 0.001 (0.143)   Loss 0.8357 (0.7050)   Prec@1 73.000 (75.303)   Prec@5 100.000 (98.398)   [2025-10-22 07:42:49]
  Epoch: [036][300/500]   Time 0.042 (0.137)   Data 0.001 (0.096)   Loss 0.8327 (0.7075)   Prec@1 70.000 (75.326)   Prec@5 95.000 (98.412)   [2025-10-22 07:42:54]
  Epoch: [036][400/500]   Time 0.047 (0.114)   Data 0.001 (0.072)   Loss 0.5997 (0.7071)   Prec@1 78.000 (75.334)   Prec@5 98.000 (98.434)   [2025-10-22 07:42:58]
  **Train** Prec@1 75.268 Prec@5 98.402 Error@1 24.732
  **Test** Prec@1 77.160 Prec@5 98.680 Error@1 22.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:43:24] [Epoch=037/040] [Need: 00:05:20] [LR=0.0010] [Best : Accuracy=77.16, Error=22.84]
  Epoch: [037][000/500]   Time 26.316 (26.316)   Data 26.264 (26.264)   Loss 0.6593 (0.6593)   Prec@1 77.000 (77.000)   Prec@5 98.000 (98.000)   [2025-10-22 07:43:51]
  Epoch: [037][100/500]   Time 0.044 (0.302)   Data 0.001 (0.261)   Loss 0.9032 (0.7055)   Prec@1 72.000 (75.267)   Prec@5 95.000 (98.535)   [2025-10-22 07:43:55]
  Epoch: [037][200/500]   Time 0.050 (0.179)   Data 0.001 (0.132)   Loss 0.6931 (0.6988)   Prec@1 79.000 (75.667)   Prec@5 96.000 (98.393)   [2025-10-22 07:44:00]
  Epoch: [037][300/500]   Time 0.055 (0.143)   Data 0.001 (0.088)   Loss 0.5919 (0.7011)   Prec@1 72.000 (75.688)   Prec@5 100.000 (98.382)   [2025-10-22 07:44:08]
  Epoch: [037][400/500]   Time 0.076 (0.125)   Data 0.002 (0.067)   Loss 0.5726 (0.7025)   Prec@1 79.000 (75.673)   Prec@5 97.000 (98.359)   [2025-10-22 07:44:15]
  **Train** Prec@1 75.568 Prec@5 98.368 Error@1 24.432
  **Test** Prec@1 77.200 Prec@5 98.630 Error@1 22.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 07:44:58] [Epoch=038/040] [Need: 00:03:33] [LR=0.0010] [Best : Accuracy=77.20, Error=22.80]
  Epoch: [038][000/500]   Time 37.308 (37.308)   Data 37.255 (37.255)   Loss 0.7926 (0.7926)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-22 07:45:36]
  Epoch: [038][100/500]   Time 0.043 (0.410)   Data 0.001 (0.370)   Loss 0.5162 (0.6929)   Prec@1 81.000 (75.673)   Prec@5 99.000 (98.525)   [2025-10-22 07:45:40]
  Epoch: [038][200/500]   Time 0.037 (0.227)   Data 0.001 (0.186)   Loss 0.8356 (0.6948)   Prec@1 69.000 (75.498)   Prec@5 98.000 (98.458)   [2025-10-22 07:45:44]
  Epoch: [038][300/500]   Time 0.040 (0.165)   Data 0.001 (0.125)   Loss 0.4982 (0.7022)   Prec@1 84.000 (75.319)   Prec@5 100.000 (98.429)   [2025-10-22 07:45:48]
  Epoch: [038][400/500]   Time 0.086 (0.136)   Data 0.001 (0.094)   Loss 0.7357 (0.7006)   Prec@1 74.000 (75.414)   Prec@5 98.000 (98.456)   [2025-10-22 07:45:53]
  **Train** Prec@1 75.312 Prec@5 98.426 Error@1 24.688
  **Test** Prec@1 77.140 Prec@5 98.710 Error@1 22.860

==>>[2025-10-22 07:46:37] [Epoch=039/040] [Need: 00:01:46] [LR=0.0010] [Best : Accuracy=77.20, Error=22.80]
  Epoch: [039][000/500]   Time 37.537 (37.537)   Data 37.489 (37.489)   Loss 0.8126 (0.8126)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-22 07:47:14]
  Epoch: [039][100/500]   Time 0.041 (0.413)   Data 0.001 (0.372)   Loss 0.7228 (0.7114)   Prec@1 76.000 (74.851)   Prec@5 99.000 (98.317)   [2025-10-22 07:47:18]
  Epoch: [039][200/500]   Time 0.036 (0.229)   Data 0.001 (0.187)   Loss 0.6393 (0.7084)   Prec@1 76.000 (75.224)   Prec@5 99.000 (98.423)   [2025-10-22 07:47:23]
  Epoch: [039][300/500]   Time 0.049 (0.166)   Data 0.001 (0.125)   Loss 0.4902 (0.7016)   Prec@1 85.000 (75.302)   Prec@5 99.000 (98.508)   [2025-10-22 07:47:26]
  Epoch: [039][400/500]   Time 0.055 (0.136)   Data 0.001 (0.094)   Loss 0.8038 (0.7016)   Prec@1 72.000 (75.329)   Prec@5 99.000 (98.536)   [2025-10-22 07:47:31]
  **Train** Prec@1 75.226 Prec@5 98.490 Error@1 24.774
  **Test** Prec@1 77.280 Prec@5 98.610 Error@1 22.720
=> Obtain best accuracy, and update the best model
